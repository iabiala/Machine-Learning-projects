{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT9DaKaet90POnh3gYZi/h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["As the agent observes the current state of the environment and chooses an action, the environment transitions into a new state, and also returns a reward that indicates the consequences of the action"],"metadata":{"id":"EdHUu2eum-I8"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxGJEMkgm14B","executionInfo":{"status":"ok","timestamp":1770778418262,"user_tz":300,"elapsed":4427,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}},"outputId":"cd672dab-90d7-4075-d718-640ae1a8a4fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (3.1.2)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (0.0.4)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic_control]) (2.6.1)\n"]}],"source":["pip install gymnasium[classic_control]"]},{"cell_type":"markdown","source":["This implementation uses neural networks `torch.nn`, optimization `torch.optim`, and automatic differentiation `torch.autograd`"],"metadata":{"id":"KsAPXbYknx6v"}},{"cell_type":"code","source":["import gymnasium as gym\n","import math\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","# set up matplotlib\n","is_ipython = \"inline\" in matplotlib.get_backend()\n","if is_ipython:\n","  from IPython import display\n","\n","plt.ion()\n","\n","seed = 42\n","random.seed(seed)\n","torch.manual_seed(seed)\n","env.reset(seed=seed)\n","env.action_space.seed(seed)\n","env.observation_space.seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(seed)"],"metadata":{"id":"y1HSz7Q6nmsW","executionInfo":{"status":"ok","timestamp":1770778947984,"user_tz":300,"elapsed":107,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Initialize Replay Memory or Buffer**\n","\n","It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It greatly stabilizes and improves the DQN training procedure.\n","\n","Two classes are needed: `Transition` and `ReplayMemory`"],"metadata":{"id":"8kmV5r1fqBNq"}},{"cell_type":"code","source":["Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n","\n","class ReplayMemory(object):\n","  def __init__(self, capacity):\n","    self.memory = deque([], maxlen=capacity)\n","\n","  def push(self, *args):\n","    \"\"\"Save a transition\"\"\"\n","    self.memory.append(Transition(*args))\n","\n","  def sample(self, batch_size):\n","    return random.sample(self.memory, batch_size)\n","\n","  def __len__(self):\n","    return len(self.memory)"],"metadata":{"id":"GZ99ZQMArSDj","executionInfo":{"status":"ok","timestamp":1770779740763,"user_tz":300,"elapsed":7,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["The main idea behind Q-learning is that if we had a function\n","Q\n","∗\n",":\n","S\n","t\n","a\n","t\n","e\n","×\n","A\n","c\n","t\n","i\n","o\n","n\n","→\n","R\n","Q\n","∗\n"," :State×Action→R\n",", that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n","\n","π\n","∗\n","(\n","s\n",")\n","=\n","arg\n","⁡\n","max\n","⁡\n","a\n","\n","Q\n","∗\n","(\n","s\n",",\n","a\n",")\n","π\n","∗\n"," (s)=arg\n","a\n","max\n","​\n","  Q\n","∗\n"," (s,a)\n","However, we don’t know everything about the world, so we don’t have access to\n","Q\n","∗\n","Q\n","∗\n","\n",". But, since neural networks are universal function approximators, we can simply create one and train it to resemble\n","Q\n","∗\n","Q\n","∗\n","\n","."],"metadata":{"id":"Ev8ZZ4l3uGYS"}},{"cell_type":"code","source":["class DQN(nn.Module):\n","\n","  def __init__(self, n_observations, n_actions):\n","    super(DQN, self).__init__()\n","    self.layer1 = nn.Linear(n_observations, 128)\n","    self.layer2 = nn.Linear(128, 128)\n","    self.layer3 = nn.Linear(128, n_actions)\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer1(x))\n","    x = F.relu(self.layer2(x))\n","    return self.layer3(x)"],"metadata":{"id":"KjrjqmZ2uPod","executionInfo":{"status":"ok","timestamp":1770780434070,"user_tz":300,"elapsed":11,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Training**\n","\n","The hyperparameter and optimizers are instantiated for our model"],"metadata":{"id":"iX9PO-aBvfJh"}},{"cell_type":"code","source":["# BATCH_SIZE is the number of transitions sampled from the replay buffer\n","# GAMMA is the discount factor as mentioned in the previous section\n","# EPS_START is the starting value of epsilon\n","# EPS_END is the final value of epsilon\n","# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n","# TAU is the update rate of the target network\n","# LR is the learning rate of the ``AdamW`` optimizer\n","\n","BATCH_SIZE = 128\n","GAMMA = 0.99\n","EPS_START = 0.9\n","EPS_END = 0.01\n","EPS_DECAY = 2500\n","TAU = 0.005\n","LR = 3e-4\n","\n","# Get number of actions from gym action space\n","n_actions = env.action_space.n\n","# Get number of state observations\n","state, info = env.reset()\n","n_observations = len(state)\n","\n","policy_net = DQN(n_observations, n_actions)\n","target_net = DQN(n_observations, n_actions)\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n","memory = ReplayMemory(10000)\n","\n","steps_done = 0\n","\n","def select_action(state):\n","  global steps_done\n","  sample = random.random()\n","  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","  steps_done += 1\n","  if sample > eps_threshold:\n","    with torch.no_grad():\n","      # t.max(1) will return the largest column value of each row.\n","      # second column on max result is index of where max element was\n","      # found, so we pick action with the larger expected reward.\n","      return policy_net(state).max(1).indices.view(1,1)\n","  else:\n","    return torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n","\n","\n","episode_durations = []\n","\n","def plot_durations(show_result=False):\n","  plt.figure(1)\n","  durations_t = torch.tensor(episode_durations, dtype= torch.float)\n","  if show_result:\n","    plt.title('Result')\n","  else:\n","    plt.clf()\n","    plt.title(\"Training...\")\n","  plt.xlabel('Episode')\n","  plt.ylabel('Duration')\n","  plt.plot(durations_t.numpy())\n","\n","  # Take 100 episode averages and plot them too\n","  if len(durations_t) >= 100:\n","    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n","    means = torch.cat(torch.zeros(99), means)\n","    plt.plot(means.numpy())\n","\n","  plt.pause(0.001) # pause a bit so that plots are updated\n","  if is_ipython:\n","    if not show_result:\n","      display.display(plt.gcf())\n","      display.clear_output(wait=True)\n","    else:\n","      display.display(plt.gcf())\n"],"metadata":{"id":"i5TYSF37vyZN","executionInfo":{"status":"ok","timestamp":1770782466759,"user_tz":300,"elapsed":5922,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**Training loop**"],"metadata":{"id":"LMbImJ5f3EwI"}},{"cell_type":"code","source":["def optimize_model():\n","  if len(memory) < BATCH_SIZE:\n","    return\n","  transitions = memory.sample(BATCH_SIZE)\n","  # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","  # detailed explanation). This converts batch-array of Transitions\n","  # to Transition of batch-arrays.\n","  batch = Transition(*zip(*transitions))\n","\n","  # Compute a mask of non-final states and concatenate the batch elements\n","  # (a final state would've been the one after which simulation ended)\n","  non_final_mask = torch.tensor(tuple(map(lambda s:s is not None, batch.next_state), dtype=torch.bool))\n","  non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n","\n","  state_batch = torch.cat(batch.state)\n","  action_batch = torch.cat(batch.action)\n","  reward_batch = torch.cat(batch.reward)\n","\n","  # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","  # columns of actions taken. These are the actions which would've been taken\n","  # for each batch state according to policy_net\n","  state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","  # Compute V(s_{t+1}) for all next states.\n","  # Expected values of actions for non_final_next_states are computed based\n","  # on the \"older\" target_net; selecting their best reward with max(1).values\n","  # This is merged based on the mask, such that we'll have either the expected\n","  # state value or 0 in case the state was final.\n","  next_state_values = torch.zeros(BATCH_SIZE)\n","  with torch.no_grad():\n","    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).next_state_values\n","  # Compute the expected Q values\n","  expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","  # Compute Huber Loss\n","  criterion = nn.SmoothL1Loss()\n","  loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","  # Optimize the model\n","  optimizer.zero_grad()\n","  loss.backward()\n","\n","  # In-place gradient clipping\n","  torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","  optimizer.step()"],"metadata":{"id":"vAir8uoL3Ded","executionInfo":{"status":"ok","timestamp":1770784600294,"user_tz":300,"elapsed":5,"user":{"displayName":"Israel Abiala","userId":"10002496334614533715"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Main Training Loop**"],"metadata":{"id":"E1Oy8GyW_YaN"}},{"cell_type":"code","source":["if torch.cuda.is_available() or torch.backends.mps.is_available():\n","    num_episodes = 600\n","else:\n","    num_episodes = 50\n","\n","for i_episode in range(num_episodes):\n","  # Initialize the environment and get its state\n","  state, info = env.reset()\n","  state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","  for t in count():\n","    action = select_action(state)\n","    observation, reward, terminated, truncated = env.step(action.item())\n","    reward = torch.tensor([reward])\n","    done = terminated or truncated\n","\n","    if terminated:\n","      next_state = None\n","    else:\n","      next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n","\n","    # Store the transition in memory\n","    memory.push(state, action, next_state, reward)\n","\n","    # Move to the next state\n","    state = next_state\n","\n","    # Perform one step of the optimizaton (on the policy network)\n","    optimize_model()\n"],"metadata":{"id":"VFrmNt99_bge"},"execution_count":null,"outputs":[]}]}